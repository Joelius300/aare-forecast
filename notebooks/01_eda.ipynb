{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from darts import TimeSeries\n",
    "\n",
    "from aare.constants import LOC_BERN, LOC_THUN, TIME, TEMP\n",
    "from aare.remote_existenz_store import RemoteExistenzStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = RemoteExistenzStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"1h\"\n",
    "ANYTIME = \"0\"  # to be used as period start when querying influx. starting at 0 just returns all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = store.query_hydro(ANYTIME, LOC_BERN, agg_freq=freq)\n",
    "o_df = df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore original df for iterative development, can re-run if necessary\n",
    "df = o_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there shouldn't be any NaNs because we set agg_create_empty to False (default)\n",
    "df.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at max one is allowed, at the end of the series\n",
    "assert (df.set_index(TIME).resample(freq).count() > 1).sum().item() <= 1, \"Has more than one data points within one time-step according to frequency\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to add nan points where data is missing. also removes the trailing data point if 18:00 and 18:55 for example.\n",
    "# doing this manually gives a bit more control and avoid having to send this data over the air from the influx server.\n",
    "df = df.set_index(TIME).resample(freq).first().reset_index(TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df, TIME, TEMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "We can clearly see that there is a lot of data missing from 2003 to 2009 although the existing data seems plausible and follows the trend.\n",
    "\n",
    "We can also clearly see some outliers that extend below the 0° border, which doesn't make sense. This is also reflected in the dataset summary, where the minimum is -9.5°.\n",
    "\n",
    "Manually creeping up on the Y-axis to see what the lowest, likely valid temperature is, it seems that 2.5° would be a good cutoff to remove outliers on the low end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Manual visual analysis shows that there is a period from Jan 2003 to Mar 2003 with very strange data. This data should be excluded.\n",
    "You could assume that this is the start of some measurement difficulties that are only remedied in 2009, so unless we find that it's not enough data, it might be best to exclude everything from Jan 2003 up to Jul 2009, where everything seems to be in order again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def between(df, from_, to_):\n",
    "    \"\"\"Returns a boolean mask for a time period selection. Assumes '_time' as time column and falls back to index.\"\"\"\n",
    "    if TIME in df.columns:\n",
    "        return (df[TIME] >= from_) & (df[TIME] < to_)\n",
    "    \n",
    "    return (df.index >= from_) & (df.index < to_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[between(df, \"2003-01-28\", \"2003-03-03\"), TEMP] = np.nan\n",
    "# this works if the time is the index\n",
    "# df.loc[\"2003-01-28\":\"2003-03-03\", TEMP] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cutoff = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Below 0°C:\", np.count_nonzero(df[TEMP] <= 0))\n",
    "print(f\"Between 0 and {low_cutoff}°C:\", np.count_nonzero((df[TEMP] > 0) & (df[TEMP] <= low_cutoff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate all data points below the lower outlier cutoff\n",
    "df.loc[df[TEMP] <= low_cutoff, TEMP] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df, TIME, TEMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "After cleanup, it appears that the data from June 10th 2009 onwards is best. The big gap with very little data from 2003 to 2009 is probably unusable.\n",
    "The data before that (Jun 2001 - Jun 2003) seems mostly usable but has some large gaps as well.\n",
    "\n",
    "It might be easier to discard just this data as it's less than 2 years worth and could have non-negligible differences in measurement methodology, distribution, etc. compared to the new data from 6 years later. Comparing (Jun 2001 - Jun 2003) to (Jun 2009 - Jun 2011) (see below) doesn't raise any warning flags that the data prior to 2009 would be invalid. However, discarding it also loses about 18 months of data and leaves us with 15 years (180 months) of continuous data; that's a 9% loss.\n",
    "\n",
    "If experimentation shows that more data would be helpful, efforts to recover & clean the data prior to 2009 can be made. To start experimentation and modelling, the 15+ years after should be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df.loc[between(df, \"2001-06-01\", \"2003-06-01\")], TIME, TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df.loc[between(df, \"2009-06-01\", \"2011-06-01\")], TIME, TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any data prior to the start date completely\n",
    "good_start_date = \"2009-06-10\"\n",
    "df.drop(df[df[TIME] < good_start_date].index, inplace=True)\n",
    "# this would work as well, but then we're working with a slice copy\n",
    "# df = df[df[TIME] >= good_start_date]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df, TIME, TEMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Visually inspecting the data we can still see\n",
    "\n",
    "* Random downward spikes of unrealistic magnitude\n",
    "* Occasional gaps\n",
    "\n",
    "Apart from those, the data seems very clean already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"temp_diff_to_prev\"] = df[TEMP].diff().abs()\n",
    "df[\"temp_diff_to_next\"] = df[TEMP].diff(-1).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_quantile = 0.999\n",
    "outlier_diff = max(df[\"temp_diff_to_prev\"].quantile(outlier_quantile), df[\"temp_diff_to_next\"].quantile(outlier_quantile))\n",
    "outlier_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"temp_diff_to_prev\"] > outlier_diff) | (df[\"temp_diff_to_next\"] > outlier_diff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"temp_diff_to_prev\"] > outlier_diff) & (df[\"temp_diff_to_next\"] > outlier_diff)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "One variant of outlier is at the start and end of measurement, so [NaN, outlier, normal measurement, ...] or reverse. This is a common pattern in industry sensor data measurements at least from my experience. \\\n",
    "A slight variation of this first variant is the case when the prev or next measurement is exactly 0 instead of NaN. \\\n",
    "Another is a random drop or spike so [normal, outlier, normal]. These have both diffs above threshold.\n",
    "\n",
    "All of these variants appear in the data.\n",
    "\n",
    "Ps. another analysis like that might be necessary after interpolating the gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variant 1a\n",
    "df.loc[(df[\"temp_diff_to_prev\"].isna() | (df[\"temp_diff_to_prev\"] == 0)) & (df[\"temp_diff_to_next\"] > outlier_diff), TEMP] = np.nan\n",
    "# variant 1b\n",
    "df.loc[(df[\"temp_diff_to_next\"].isna() | (df[\"temp_diff_to_next\"] == 0)) & (df[\"temp_diff_to_prev\"] > outlier_diff), TEMP] = np.nan\n",
    "# variant 2\n",
    "df.loc[(df[\"temp_diff_to_prev\"] > outlier_diff) & (df[\"temp_diff_to_next\"] > outlier_diff), TEMP] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update diffs because outliers have now been removed (set to NaN)\n",
    "df[\"temp_diff_to_prev\"] = df[TEMP].diff().abs()\n",
    "df[\"temp_diff_to_next\"] = df[TEMP].diff(-1).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of these look legit, although they certainly fall outside the norm\n",
    "df[(df[\"temp_diff_to_prev\"] > outlier_diff) | (df[\"temp_diff_to_next\"] > outlier_diff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df, TIME, TEMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Moving to Darts now helps with gap analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ts(df):\n",
    "    \"\"\"Transforms a dataframe into a darts TimeSeries using the predefined TIME column (or index).\"\"\"\n",
    "    if TIME in df.columns:\n",
    "        tdf = df.set_index(TIME)\n",
    "    else:\n",
    "        tdf = df\n",
    "    # turn it into timezone-naive timestamps because that's what darts wants.\n",
    "    # all the data is in UTC anyway, so a conversion is necessary on display no matter what.\n",
    "    tdf.index = tdf.index.tz_localize(None)\n",
    "    \n",
    "    return TimeSeries.from_dataframe(tdf, freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = to_ts(df)\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.gaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.gaps().value_counts(\"gap_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "Most gaps are size 1. This is also fairly visible in the data, for example at the end of 2013. The data doesn't appear to be wrong, just more sparse than it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[TEMP].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[TEMP].interpolate(limit=1).isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"temp_filled\"] = df[TEMP].interpolate(limit=1)\n",
    "df[\"was_filled\"] = df[TEMP].isna() & ~df[\"temp_filled\"].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, x=TIME, y=\"temp_filled\", color=\"was_filled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "TODO Found some wild outlier on 2009-06-06, maybe move start date a bit later (e.g. 09.06.) or manually remove that outlier (can threshold to 25 for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
