{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from darts import TimeSeries\n",
    "\n",
    "from aare.constants import LOC_BERN, LOC_THUN, TIME, TEMP\n",
    "from aare.remote_existenz_store import RemoteExistenzStore\n",
    "from aare.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = RemoteExistenzStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"1h\"\n",
    "ANYTIME = \"0\"  # to be used as period start when querying influx. starting at 0 just returns all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = store.query_hydro(ANYTIME, LOC_BERN, agg_freq=freq)\n",
    "o_df = df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore original df for iterative development, can re-run if necessary\n",
    "df = o_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there shouldn't be any NaNs because we set agg_create_empty to False (default)\n",
    "df.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at max one is allowed, at the end of the series\n",
    "assert (df.set_index(TIME).resample(freq).count() > 1).sum().item() <= 1, \"Has more than one data points within one time-step according to frequency\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to add nan points where data is missing. also removes the trailing data point if 18:00 and 18:55 for example.\n",
    "# doing this manually gives a bit more control and avoid having to send this data over the air from the influx server.\n",
    "df = df.set_index(TIME).resample(freq).first().reset_index(TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df, TIME, TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, TIME, TEMP).update_traces(marker={'size': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "We can clearly see that there is a lot of data missing from 2003 to 2009 although the existing data seems plausible and follows the seasonality and trend. It just has a lower frequency? Since px.line only connects points if there is no NaN in between, this period almost looks non-existent with a line plot, but in a scatter plot, it looks mostly fine.\n",
    "\n",
    "We can also clearly see some outliers that extend below the 0° border, which doesn't make sense. This is also reflected in the dataset summary, where the minimum is -9.5°.\n",
    "\n",
    "Manually creeping up on the Y-axis to see what the lowest likely valid temperature is, it seems that 2.5° would be a good cutoff to remove outliers on the low end.\n",
    "\n",
    "Similarly, the maximum temperature of 25° is above the highest recorded temperature according to other sources and the only occurrence of it is clearly an outlier (2009-07-06), so threshold that away as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Manual visual analysis shows that there is a period from Jan 2003 to Mar 2003 with very strange data. This data should be excluded.\n",
    "You could assume that this is the start of some measurement difficulties that are only remedied in 2009, so unless we find that it's not enough data, it might be best to exclude everything from Jan 2003 up to Jul 2009, where everything seems to be in order again.\n",
    "\n",
    "EDIT: Given that the data in those 6 years seems to be trivially salvageable, let's just run with it and interpolate where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[between(df, \"2003-01-28\", \"2003-03-03\"), TEMP] = np.nan\n",
    "# this works if the time is the index\n",
    "# df.loc[\"2003-01-28\":\"2003-03-03\", TEMP] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cutoff = 2.5\n",
    "high_cutoff = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x <= 0°C:\", np.count_nonzero(df[TEMP] <= 0))\n",
    "print(f\"0 < x <= {low_cutoff}°C:\", np.count_nonzero((df[TEMP] > 0) & (df[TEMP] <= low_cutoff)))\n",
    "print(f\"x >= {high_cutoff}°C:\", np.count_nonzero(df[TEMP] >= high_cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_mask = (df[TEMP] <= low_cutoff) | (df[TEMP] >= high_cutoff)\n",
    "px.scatter(df, x=TIME, y=TEMP, color=outlier_mask).update_traces(marker={'size': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate all data points outside valid bound\n",
    "df.loc[outlier_mask, TEMP] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, TIME, TEMP).update_traces(marker={'size': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "TODO I think I got it all wrong, I just need to interpolate a lot more from 2003 to 2009 because it has a lot of 2 hour frequency instead of 1 hour. Maybe also investigate what the actual source (influx) provides BEFORE aggregation to an hour.\n",
    "\n",
    "TODO also another type of outlier is stray data points that have no data to their left and right for more than a few hours (see Mar 2002 below).\n",
    "\n",
    "After cleanup, it appears that the data from June 10th 2009 onwards is best. The big gap with very little data from 2003 to 2009 is probably unusable.\n",
    "The data before that (Jun 2001 - Jun 2003) seems mostly usable but has some large gaps as well.\n",
    "\n",
    "It might be easier to discard just this data as it's less than 2 years worth and could have non-negligible differences in measurement methodology, distribution, etc. compared to the new data from 6 years later. Comparing (Jun 2001 - Jun 2003) to (Jun 2009 - Jun 2011) (see below) doesn't raise any warning flags that the data prior to 2009 would be invalid. However, discarding it also loses about 18 months of data and leaves us with 15 years (180 months) of continuous data; that's a 9% loss.\n",
    "\n",
    "If experimentation shows that more data would be helpful, efforts to recover & clean the data prior to 2009 can be made. To start experimentation and modelling, the 15+ years after should be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df.loc[between(df, \"2001-06-01\", \"2003-06-01\")], TIME, TEMP).update_traces(marker={'size': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df.loc[between(df, \"2009-06-01\", \"2011-06-01\")], TIME, TEMP).update_traces(marker={'size': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Visually inspecting the data we can still see\n",
    "\n",
    "* Random downward spikes of unrealistic magnitude\n",
    "* Occasional gaps\n",
    "\n",
    "Apart from those, the data seems very clean already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[TEMP].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"temp_diff_to_prev\"] = df[TEMP].diff().abs()\n",
    "df[\"temp_diff_to_next\"] = df[TEMP].diff(-1).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_quantile = 0.999\n",
    "outlier_diff = max(df[\"temp_diff_to_prev\"].quantile(outlier_quantile), df[\"temp_diff_to_next\"].quantile(outlier_quantile))\n",
    "outlier_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"temp_diff_to_prev\"] > outlier_diff) | (df[\"temp_diff_to_next\"] > outlier_diff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"temp_diff_to_prev\"] > outlier_diff) & (df[\"temp_diff_to_next\"] > outlier_diff)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "One variant of outlier is at the start and end of measurement, so [NaN, outlier, normal measurement, ...] or reverse. This is a common pattern in industry sensor data measurements at least from my experience. \\\n",
    "A slight variation of this first variant is the case when the prev or next measurement is exactly 0 instead of NaN. \\\n",
    "Another is a random drop or spike so [normal, outlier, normal]. These have both diffs above threshold.\n",
    "\n",
    "All of these variants appear in the data.\n",
    "\n",
    "Ps. another analysis like that might be necessary after interpolating the gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variant 1a\n",
    "df.loc[(df[\"temp_diff_to_prev\"].isna() | (df[\"temp_diff_to_prev\"] == 0)) & (df[\"temp_diff_to_next\"] > outlier_diff), TEMP] = np.nan\n",
    "# variant 1b\n",
    "df.loc[(df[\"temp_diff_to_next\"].isna() | (df[\"temp_diff_to_next\"] == 0)) & (df[\"temp_diff_to_prev\"] > outlier_diff), TEMP] = np.nan\n",
    "# variant 2\n",
    "df.loc[(df[\"temp_diff_to_prev\"] > outlier_diff) & (df[\"temp_diff_to_next\"] > outlier_diff), TEMP] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update diffs because outliers have now been removed (set to NaN)\n",
    "df[\"temp_diff_to_prev\"] = df[TEMP].diff().abs()\n",
    "df[\"temp_diff_to_next\"] = df[TEMP].diff(-1).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of these look legit, although they certainly fall outside the norm\n",
    "df[(df[\"temp_diff_to_prev\"] > outlier_diff) | (df[\"temp_diff_to_next\"] > outlier_diff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, TIME, TEMP).update_traces(marker={'size': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Moving to Darts now helps with gap analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = to_ts(df)\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.gaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.gaps().value_counts(\"gap_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Most gaps are size 1. This is also fairly visible in the data, for example at the end of 2013. The data doesn't appear to be wrong, just more sparse than it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[TEMP].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[TEMP].interpolate(limit=1).isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"temp_filled\"] = df[TEMP].interpolate(limit=1)\n",
    "df[\"was_filled\"] = (df[TEMP].isna() & ~df[\"temp_filled\"].isna()).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, x=TIME, y=\"temp_filled\", color=\"was_filled\").update_traces(marker={'size': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"temp_filled0\"] = df[TEMP]\n",
    "df[\"was_filled\"] = 0\n",
    "df[\"was_filled\"] = df[\"was_filled\"].astype(np.uint8)\n",
    "# multiple steps of interpolation with just limit 1 to see where the larger gaps are\n",
    "for i in range(1, 25):\n",
    "    p, c = f\"temp_filled{i-1}\", f\"temp_filled{i}\"\n",
    "    df[c] = df[p].interpolate(limit=1, limit_area=\"inside\")\n",
    "    df[\"was_filled\"] += i * (df[p].isna() & ~df[c].isna()).astype(np.uint8)\n",
    "    df[\"temp_filled\"] = df[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, x=TIME, y=\"temp_filled\", color=\"was_filled\").update_traces(marker={'size': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "TODO Found some wild outlier on 2009-07-06, maybe move start date a bit later (e.g. 09.07.) or manually remove that outlier (can threshold to 25 for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"temp_filled0\"] = df[TEMP]\n",
    "df[\"was_filled\"] = 0\n",
    "df[\"was_filled\"] = df[\"was_filled\"].astype(np.uint8)\n",
    "# multiple steps of interpolation with just limit 1 to see where the larger gaps are\n",
    "for i in range(1, 25):\n",
    "    p, c = f\"temp_filled{i-1}\", f\"temp_filled{i}\"\n",
    "    df[c] = df[p].interpolate(method=\"cubic\", limit=1, limit_area=\"inside\")\n",
    "    df[\"was_filled\"] += i * (df[p].isna() & ~df[c].isna()).astype(np.uint8)\n",
    "    df[\"temp_filled\"] = df[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, x=TIME, y=\"temp_filled\", color=\"was_filled\").update_traces(marker={'size': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = fill_with_hard_limit(df, limit=5, columns=[TEMP], add_was_filled=True)\n",
    "px.scatter(df_i, x=TIME, y=TEMP, color=TEMP+\"_filled\").update_traces(marker={'size': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "TODO Outlier 2012-08-04, must be removed manually like the other one in the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "Manually scanning the plot shows that linear interpolation is fine for gaps up to 5 hours. This could be tuned and done much more rigorously, but it's not really necessary here and likely only yields marginal benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"temp_filled\"].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"was_filled\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_ts(df_i[[TIME, TEMP]]).gaps().sort_values(\"gap_size\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "Incorporate this interpolation into the real dataset and keep track of which portions were filled how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[TEMP] = df_i[TEMP]\n",
    "df[\"filled\"] = df_i[TEMP+\"_filled\"].map({ False: \"none\", True: \"linear\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, x=TIME, y=TEMP, color=\"filled\").update_traces(marker={'size': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "Given the remaining few gaps, a maximum gap size of 25 or 30 seems appropriate to be filled with a slightly more complex method. \\\n",
    "The results are promising because the trend is retained well in March 2022 for those shorter gaps, but the larger gap in March 2010 would need two cycles/humps, which would not be filled well with a cubic approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = fill_with_hard_limit(df, method=\"cubic\", limit=25, columns=[TEMP], add_was_filled=True)\n",
    "px.scatter(df_i, x=TIME, y=TEMP, color=TEMP + \"_filled\").update_traces(marker={'size': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_ts(df_i[[TIME, TEMP]]).gaps().sort_values(\"gap_size\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[TEMP] = df_i[TEMP]\n",
    "df.loc[df_i[TEMP+\"_filled\"], \"filled\"] = \"cubic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, x=TIME, y=TEMP, color=\"filled\").update_traces(marker={'size': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = fill_with_hard_limit(df, method=\"akima\", limit=200, columns=[TEMP], add_was_filled=True)\n",
    "px.scatter(df_i, x=TIME, y=TEMP, color=TEMP + \"_filled\").update_traces(marker={'size': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "Unfortunately, the large gaps are hard to fill with the methods of interpolate; none of them seem to capture patterns plausibly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "TODO Whether more interpolation should be done depends on the forecasting method to use. When training on multiple series isn't possible, you must fill _somehow_ anyway. If it is possible, you just lose some fixed amount of extra data per gap because the gap cannot be contained in the used lags, nor the predicted horizon (because of validation).\n",
    "\n",
    "Could also use a simple model to interpolate, then use that \"good enough\" data for more complex modelling. The amount of non-trivially fillable missing data is small.\n",
    "\n",
    "After some research, using PyPots and BRITS to impute dataset is probably best. Filling very small gaps with linear is still acceptable probably, but might as well try to do it entirely with BRITS and see how that performs. There are other models available, but it seems that BRITS would perform well on a dataset like this, given the benchmarking done by the PyPots devs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "Also, instead of getting carried away with imputation, you should start looking at seasonality etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
